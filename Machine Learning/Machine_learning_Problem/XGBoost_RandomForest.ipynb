{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost stands for “Extreme Gradient Boosting”, where the term “Gradient Boosting” originates from the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman.\n",
    "\n",
    "### First talk Ensemble\n",
    "\n",
    "![](https://camo.githubusercontent.com/b82e1c0a90068b3098a75655c1667e6fe9db07c1/687474703a2f2f696d616765732e736c696465706c617965722e636f6d2f33372f31303734373831342f736c696465732f736c6964655f322e6a7067)\n",
    "\n",
    "* An ensemble is just a collection of predictors which come together (e.g. mean of all predictions) to give a final prediction.\n",
    "* The reason we use ensembles is that many different predictors trying to predict same target variable will perform a better job than any single predictor alone.\n",
    "* Ensembling techniques are further classified into Bagging and Boosting.\n",
    "\n",
    "#### What's Bagging?\n",
    "\n",
    "* Bagging is a simple ensembling technique in which we build many independent predictors/models/learners and combine them using some model averaging techniques. (e.g. weighted average, majority vote or normal average)\n",
    "* We typically take random sub-sample/bootstrap of data for each model, so that all the models are little different from each other.\n",
    "* Each observation is chosen with replacement to be used as input for each of the model. So, each model will have different observations based on the bootstrap process.\n",
    "* Because this technique takes many uncorrelated learners to make a final model, it reduces error by reducing variance. Example of bagging ensemble is Random Forest models.\n",
    "\n",
    "![](https://camo.githubusercontent.com/22cf4bccd4e6910fe5858118bb371dda3565a415/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3539322f312a69306f386d6a4666436e2d754437392d463143716b772e706e67)\n",
    "\n",
    "![](https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/cart.png)\n",
    "\n",
    "### What's Boosting?\n",
    "\n",
    "* Boosting is an ensemble technique in which the predictors are not made independently, but sequentially.\n",
    "* subsequent predictors learn from the mistakes of the previous predictors.\n",
    "* Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most.\n",
    "* So the observations are not chosen based on the bootstrap process, but based on the error -\n",
    "* The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc.\n",
    "* Because new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions.\n",
    "* But we have to choose the stopping criteria carefully or it could lead to overfitting on training data.\n",
    "\n",
    "#### Gradient Boosting is an type of boosting algorithm!\n",
    "\n",
    "![](https://camo.githubusercontent.com/571661ba88ade393d82ec85fbeaccda39021d347/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a38543448456a7a48746f5f5638507245464c6b6439412e706e67)\n",
    "\n",
    "![](https://camo.githubusercontent.com/5319f660d57bb9915ce2b1da5382fe2d8bbb2dae/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a5061584a38484359453972324d67695a3332545132412e706e67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of any supervised learning algorithm is to define a loss function and minimize it. Let’s see how maths work out for Gradient Boosting algorithm. Say we have mean squared error (MSE) as loss defined as:\n",
    "\n",
    "![](https://camo.githubusercontent.com/aa54a589419ce12825809bd4e8e561ee411f74a6/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a6648656e6e374e567163577677323544332d7a5269512e706e67)\n",
    "\n",
    "We want our predictions, such that our loss function (MSE) is minimum. By using gradient descent and updating our predictions based on a learning rate, we can find the values where MSE is minimum.\n",
    "\n",
    "![](https://camo.githubusercontent.com/3fd194928971934fd8bf69359e8a07c21dd73b0e/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a4c4c624334547374717a585133687a413877436d65672e706e67)\n",
    "\n",
    "So, we are basically updating the predictions such that the sum of our residuals is close to 0 (or minimum) and predicted values are sufficiently close to actual values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
